{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMV0r5iUkRyWEN60Mctglk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravKanwat/CS6910_Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "qMxSuxAsu1R7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "wandb.login(key=\"0f6963d23192cbab4399ad9ec6e7475c7a0d6345\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6EJ98EdjzyE",
        "outputId": "35dd4854-7fe6-46a4-deef-9c1a67d6fb28"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def printImages(x_train, y_train):\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  index = [list(y_train).index(i) for i in range(len(classes))]\n",
        "\n",
        "  # image --> An image in a class; labels --> label\n",
        "  images = []\n",
        "  labels = []\n",
        "  for i in index:\n",
        "    images.append(x_train[i])\n",
        "    labels.append(classes[y_train[i]])\n",
        "  wandb.log({\"Images\": [wandb.Image(image, caption=caption) for image, caption in zip(images, labels)]})\n",
        ""
      ],
      "metadata": {
        "id": "g7koOpFuj2YA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "      self.num_of_pixels = num_of_pixels\n",
        "      self.hidden_neurons_list = hidden_neurons_list\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.output_neurons = output_neurons\n",
        "\n",
        "\n",
        "    def initialize_parameters(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons, initialization):\n",
        "\n",
        "      weights = {}\n",
        "      biases = {}\n",
        "      #num_layers = len(hidden_neurons_list) + 1\n",
        "\n",
        "      if initialization == \"xavier\":\n",
        "        weights[0] = np.random.randn(hidden_neurons_list[0], num_of_pixels) * np.sqrt(1 / num_of_pixels)\n",
        "        biases[0] = np.zeros((hidden_neurons_list[0], 1))\n",
        "\n",
        "        # Initialize weights and biases for hidden layers\n",
        "        for l in range(1, len(hidden_neurons_list)):\n",
        "          weights[l] = np.random.randn(hidden_neurons_list[l], hidden_neurons_list[l-1]) * np.sqrt(1 / hidden_neurons_list[l-1])\n",
        "          biases[l] = np.zeros((hidden_neurons_list[l], 1))\n",
        "\n",
        "        # Initialize weights for last hidden layer to output layer\n",
        "        weights[len(hidden_neurons_list)] = np.random.randn(output_neurons, hidden_neurons_list[-1]) * np.sqrt(1 / hidden_neurons_list[-1])\n",
        "        biases[len(hidden_neurons_list)] = np.zeros((output_neurons, 1))\n",
        "        return weights, biases\n",
        "\n",
        "      #random initialization\n",
        "      else:\n",
        "        weights[0] = np.random.rand(hidden_neurons_list[0], num_of_pixels) - 0.5\n",
        "        biases[0] = np.random.rand(hidden_neurons_list[0], 1) - 0.5\n",
        "        for l in range(num_hidden_layers):\n",
        "          weights[l] = np.random.rand(hidden_neurons_list[l], num_of_pixels if l == 0 else hidden_neurons_list[l-1]) - 0.5\n",
        "          biases[l] = np.random.rand(hidden_neurons_list[l], 1) - 0.5\n",
        "        weights[num_hidden_layers] = np.random.rand(output_neurons, hidden_neurons_list[-1]) - 0.5\n",
        "        biases[num_hidden_layers] = np.random.rand(output_neurons, 1) - 0.5\n",
        "      return weights, biases\n",
        "      '''\n",
        "          Initializing the weights and biases, both are dictionary which are storing random values generated by rand between (0 to 1) and subtracting 0.5 from it makes it between\n",
        "          -0.5 to 0.5\n",
        "      '''\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "      sigmoid_x = np.where(x < -30, 1, 1 / (1 + np.exp(-x)))\n",
        "      return sigmoid_x\n",
        "\n",
        "    def reLU(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "    def identity(self, x):\n",
        "      return x\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_x = np.max(x, axis=0)\n",
        "        exp_x = np.exp(x - max_x)  # avoiding overflow\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "    def feedforward_propagation(self, X, weights, biases, num_hidden_layers, activation_function):\n",
        "      a = []\n",
        "      h = []\n",
        "\n",
        "      for k in range(num_hidden_layers):\n",
        "        if k == 0:\n",
        "          a.append(np.dot(weights[k], X) + biases[k])\n",
        "          if(activation_function == \"reLU\"):\n",
        "            h.append(self.reLU(a[k]))\n",
        "          elif(activation_function == \"sigmoid\"):\n",
        "            h.append(self.sigmoid(a[k]))\n",
        "          elif(activation_function == \"tanh\"):\n",
        "            h.append(self.tanh(a[k]))\n",
        "          elif(activation_function == \"identity\"):\n",
        "            h.append(self.identity(a[k]))\n",
        "        else:\n",
        "          a.append(np.dot(weights[k], h[k-1]) + biases[k])\n",
        "          if(activation_function == \"reLU\"):\n",
        "            h.append(self.reLU(a[k]))\n",
        "          elif(activation_function == \"sigmoid\"):\n",
        "            h.append(self.sigmoid(a[k]))\n",
        "          elif(activation_function == \"tanh\"):\n",
        "            h.append(self.tanh(a[k]))\n",
        "          elif(activation_function == \"identity\"):\n",
        "            h.append(self.identity(a[k]))\n",
        "\n",
        "      a.append(np.dot(weights[num_hidden_layers], h[num_hidden_layers - 1]) + biases[num_hidden_layers])\n",
        "      y_hat = self.softmax(a[-1])\n",
        "      return a, h, y_hat\n",
        "\n",
        "    def one_hot(self, Y):\n",
        "      if Y.max() != 9:\n",
        "        one_hot_Y = np.zeros((Y.size, 10))\n",
        "      else:\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "      one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "      one_hot_Y = one_hot_Y.T\n",
        "      return one_hot_Y\n",
        "\n",
        "    def deriv_sigmoid(self, Z):\n",
        "      func = self.sigmoid(Z)\n",
        "      return func * (1 - func)\n",
        "\n",
        "    def deriv_reLU(self, Z):\n",
        "      return Z > 0\n",
        "\n",
        "    def deriv_tanh(self, x):\n",
        "      return 1 - np.tanh(x)**2\n",
        "      # sechX = 1 / np.cosh(x)\n",
        "      # return sechX ** 2\n",
        "\n",
        "    def deriv_identity(self, x):\n",
        "      return 1\n",
        "\n",
        "    def back_propagation(self, Y, fwd_A, fwd_H, weights, biases, pred_output, num_hidden_layers, activation_function):\n",
        "      one_hot_Y = self.one_hot(Y)\n",
        "      dA = {}\n",
        "      dH = {}\n",
        "      dW = {}\n",
        "      dB = {}\n",
        "\n",
        "      dA[num_hidden_layers] = pred_output - one_hot_Y\n",
        "\n",
        "      for k in range(num_hidden_layers, 0, -1):\n",
        "        dW[k] = np.dot(dA[k], fwd_H[k-1].T)\n",
        "        dB[k] = np.mean(dA[k], axis=1, keepdims=True)\n",
        "\n",
        "        dH[k-1] = np.dot(weights[k].T, dA[k])\n",
        "        if(activation_function == \"reLU\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_reLU(fwd_A[k-1]))\n",
        "        elif(activation_function == \"sigmoid\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_sigmoid(fwd_A[k-1]))\n",
        "        elif(activation_function == \"tanh\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_tanh(fwd_A[k-1]))\n",
        "        elif(activation_function == \"identity\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_identity(fwd_A[k-1]))\n",
        "      return dW, dB\n",
        "\n",
        "    def get_predictions(self, pred_output):\n",
        "      return np.argmax(pred_output, axis = 0)\n",
        "\n",
        "    def get_accuracy(self, y_pred, y_true):\n",
        "      return np.sum(y_pred == y_true) / y_true.size\n",
        "\n",
        "    def loss_function(self, y_pred, y_true, loss, weights, weight_decay):\n",
        "      if(loss == 'cross_entropy'):\n",
        "        epsilon = 1e-30\n",
        "        cross_entropy_loss = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=0))\n",
        "        reg_loss = 0.5 * weight_decay * sum(np.sum(w ** 2) for w in weights.values())\n",
        "        total_loss = cross_entropy_loss + reg_loss\n",
        "      elif(loss == 'mse'):\n",
        "        mse_loss = np.mean(np.sum((y_pred - y_true) ** 2))\n",
        "        reg_loss = 0.5 * weight_decay * sum(np.sum(w ** 2) for w in weights.values())\n",
        "        total_loss = mse_loss + reg_loss\n",
        "      # print(total_loss)\n",
        "      return total_loss\n",
        "\n",
        "    def gradient_descent(self, weights, biases, dW, dB, eta):\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        weights[l] -= eta * dW[l]\n",
        "        biases[l] -= eta * dB[l]\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def momentum_based_gradient_descent(self, weights, biases, dW, dB, epochs, eta, beta):\n",
        "      prev_uw = {}\n",
        "      prev_ub = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        prev_uw[l] = 0\n",
        "        prev_ub[l] = 0\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        uw = beta * prev_uw[l] + eta * dW[l]\n",
        "        ub = beta * prev_ub[l] + eta * dB[l]\n",
        "        weights[l] -= uw\n",
        "        biases[l] -= ub\n",
        "        prev_uw[l] = uw\n",
        "        prev_ub[l] = ub\n",
        "      return weights, biases\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, weights, biases, dW, dB, epochs, eta, beta):\n",
        "      prev_vw = 0\n",
        "      prev_vb = 0\n",
        "      v_w = beta*prev_vw\n",
        "      v_b = beta*prev_vb\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        vw = beta * prev_vw + eta * dW[l]\n",
        "        vb = beta * prev_vb + eta * dB[l]\n",
        "        weights[l] -= vw\n",
        "        biases[l] -= vb\n",
        "        prev_uw = vw\n",
        "        prev_ub = vb\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def adagrad_gradient_descent(self, weights, biases, dW, dB, eta, eps):\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = v_w[l] + dW[l]**2\n",
        "        v_b[l] = v_b[l] + dB[l]**2\n",
        "        weights[l] -= eta * dW[l] / (np.sqrt(v_w[l]) + eps)\n",
        "        biases[l] -= eta * dB[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def rmsProp_gradient_descent(self, weights, biases, dW, dB, epochs, eta, eps, beta):\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = (beta * v_w[l]) + ((1-beta) * dW[l] ** 2)\n",
        "        v_b[l] = (beta * v_b[l]) + ((1-beta) * dB[l] ** 2)\n",
        "\n",
        "        weights[l] -= eta * dW[l] / (np.sqrt(v_w[l]) + eps)\n",
        "        biases[l] -= eta * dB[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def adam_gradient_descent(self, weights, biases, dW, dB, epochs, eta, eps, beta1, beta2):\n",
        "      m_w = {}\n",
        "      m_b = {}\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "      m_w_hat = {}\n",
        "      m_b_hat = {}\n",
        "      v_w_hat = {}\n",
        "      v_b_hat = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = 0\n",
        "        m_b[l] = 0\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "        m_w_hat[l] = 0\n",
        "        m_b_hat[l] = 0\n",
        "        v_w_hat[l] = 0\n",
        "        v_b_hat[l] = 0\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = (beta1 * m_w[l]) + (1-beta1) * dW[l]\n",
        "        m_b[l] = (beta1 * m_b[l]) + (1-beta1) * dB[l]\n",
        "\n",
        "        v_w[l] = beta2 * v_w[l] + (1 - beta2) * (dW[l] ** 2)\n",
        "        v_b[l] = beta2 * v_b[l] + (1 - beta2) * (dB[l] ** 2)\n",
        "\n",
        "        m_w_hat[l] = m_w[l]/(1-np.power(beta1, l+1))\n",
        "        m_b_hat[l] = m_b[l]/(1-np.power(beta1, l+1))\n",
        "        v_w_hat[l] = v_w[l]/(1-np.power(beta2, l+1))\n",
        "        v_b_hat[l] = v_b[l]/(1-np.power(beta2, l+1))\n",
        "\n",
        "        #update parameters\n",
        "        weights[l] -= eta*m_w_hat[l]/(np.sqrt(v_w_hat[l])+eps)\n",
        "        biases[l] -= eta*m_b_hat[l]/(np.sqrt(v_b_hat[l])+eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def nadam_gradient_descent(self, weights, biases, dW, dB, epochs, eta, eps, beta1, beta2):\n",
        "      m_w = {}\n",
        "      m_b = {}\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "      m_w_hat = {}\n",
        "      m_b_hat = {}\n",
        "      v_w_hat = {}\n",
        "      v_b_hat = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = 0\n",
        "        m_b[l] = 0\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "        m_w_hat[l] = 0\n",
        "        m_b_hat[l] = 0\n",
        "        v_w_hat[l] = 0\n",
        "        v_b_hat[l] = 0\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = (beta1 * m_w[l]) + (1-beta1) * dW[l]\n",
        "        m_b[l] = (beta1 * m_b[l]) + (1-beta1) * dB[l]\n",
        "\n",
        "        v_w[l] = beta2 * v_w[l] + (1 - beta2) * (dW[l] ** 2)\n",
        "        v_b[l] = beta2 * v_b[l] + (1 - beta2) * (dB[l] ** 2)\n",
        "\n",
        "        m_w_hat[l] = m_w[l]/(1-np.power(beta1, l+1))\n",
        "        m_b_hat[l] = m_b[l]/(1-np.power(beta1, l+1))\n",
        "        v_w_hat[l] = v_w[l]/(1-np.power(beta2, l+1))\n",
        "        v_b_hat[l] = v_b[l]/(1-np.power(beta2, l+1))\n",
        "\n",
        "        #update parameters\n",
        "        weights[l] -= (eta/np.sqrt(v_w_hat[l] + eps)) * (beta1 * m_w_hat[l] + (1-beta1) * dW[l] / (1-np.power(beta1, l+1)))\n",
        "        biases[l] -= (eta/np.sqrt(v_b_hat[l] + eps)) * (beta1 * m_b_hat[l] + (1-beta1) * dB[l] / (1-np.power(beta1, l+1)))\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def compute_accuracy(self, X_test, y_test, weights, biases, num_hidden_layers, activation_function):\n",
        "\n",
        "      _, _, pred_output = self.feedforward_propagation(X_test, weights, biases, num_hidden_layers, activation_function)\n",
        "      pred_labels = np.argmax(pred_output, axis=0)\n",
        "      accuracy = np.mean(pred_labels == y_test)\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "-iMXMbOjj4LU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_neural_network(nn, x_train_input, y_train, x_test_input, y_test, x_val, y_val, weights, biases, num_hidden_layers, activation_function, optimizer, epochs, batch_size, eta, beta, beta1, beta2, eps, weight_decay, loss):\n",
        "  data_size = len(x_train_input[0])\n",
        "\n",
        "  if optimizer == \"sgd\":\n",
        "    batch_size = 1\n",
        "\n",
        "  for iter in tqdm(range(epochs)):\n",
        "    total_train_loss = 0\n",
        "    for i in range(0, data_size, batch_size):\n",
        "      if i<= data_size - batch_size:\n",
        "        X_batch = x_train_input[:, i:i+batch_size]\n",
        "        Y_batch = y_train[i:i+batch_size]\n",
        "        val_X_batch = x_val[:, i:i+batch_size]\n",
        "        val_Y_batch = y_val[i:i+batch_size]\n",
        "\n",
        "        if optimizer == \"sgd\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.gradient_descent(weights, biases, dW, dB, eta)\n",
        "\n",
        "        elif optimizer == \"momentum\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.momentum_based_gradient_descent(weights, biases, dW, dB, epochs, eta, beta)\n",
        "\n",
        "        elif optimizer == \"nesterov\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.nesterov_accelerated_gradient_descent(weights, biases, dW, dB, epochs, eta, beta)\n",
        "\n",
        "        elif optimizer == \"rmsProp\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.rmsProp_gradient_descent(weights, biases, dW, dB, epochs, eta, eps, beta)\n",
        "\n",
        "        elif optimizer == \"adam\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.adam_gradient_descent(weights, biases, dW, dB, epochs, eta, eps, beta1, beta2)\n",
        "\n",
        "        elif optimizer == \"nadam\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.nadam_gradient_descent(weights, biases, dW, dB, epochs, eta, eps, beta1, beta2)\n",
        "\n",
        "    avg_train_loss = total_train_loss / (data_size / batch_size)\n",
        "\n",
        "    _, _, val_pred = nn.feedforward_propagation(x_val, weights, biases, num_hidden_layers, activation_function)\n",
        "    val_one_hot = nn.one_hot(y_val)\n",
        "    val_loss = nn.loss_function(val_pred, val_one_hot, loss, weights, weight_decay)\n",
        "    if(loss == 'mse'):\n",
        "      val_loss = val_loss / (data_size / batch_size)\n",
        "    val_accuracy = nn.compute_accuracy(x_val, y_val, weights, biases, num_hidden_layers, activation_function)\n",
        "    train_accuracy = nn.compute_accuracy(x_train_input, y_train, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "    print(f\"val accuracy: {val_accuracy * 100:.2f}%, Train Loss: {avg_train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
        "    wandb.log({'val_accuracy' : val_accuracy * 100, 'accuracy' : train_accuracy * 100, 'loss' : avg_train_loss, 'val loss' : val_loss, 'epoch' : iter}, step=iter)\n",
        "\n",
        "  return weights, biases"
      ],
      "metadata": {
        "id": "Hldi3YHaj4Sj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "  fashion_mnist = keras.datasets.fashion_mnist\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=10000, random_state=42)\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  x_train_norm = x_train / 255\n",
        "  x_test_norm = x_test / 255\n",
        "  x_val_norm = x_val / 255\n",
        "\n",
        "\n",
        "  # Define hyperparameters\n",
        "  num_of_pixels = 28 * 28                                                         #28 * 28 = 784 pixels\n",
        "  # num_hidden_layers = 4\n",
        "  # num_hidden_neurons = 128\n",
        "  # hidden_neurons_list = []\n",
        "  # for i in range(num_hidden_layers):\n",
        "  #  hidden_neurons_list.append(num_hidden_neurons)\n",
        "  output_neurons = 10\n",
        "  # eta = 0.001\n",
        "  # epochs = 10\n",
        "  # activation_function = \"tanh\"\n",
        "  # initialization = \"normal\"\n",
        "  # optimizer = \"adam\"\n",
        "  # batch_size = 64\n",
        "  beta = 0.9\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "  # weight_decay = 0.0005\n",
        "  # loss = \"cross_entropy\"\n",
        "\n",
        "  sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'name' : 'Cross Entropy vs MSE',\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'epochs': {\n",
        "            'values' : [10]\n",
        "        },\n",
        "        'eta' : {\n",
        "            'values' : [1e-3]\n",
        "        },\n",
        "        'num_hidden_layers':{\n",
        "            'values' : [4]\n",
        "        },\n",
        "         'batch_size':{\n",
        "            'values' : [64]\n",
        "        },\n",
        "        'activation_function': {\n",
        "            'values' : ['tanh']\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values' : ['adam']\n",
        "        },\n",
        "        'num_hidden_neurons' : {\n",
        "            'values' : [128]\n",
        "        },\n",
        "        'initialization' : {\n",
        "            'values' : ['normal']\n",
        "        },\n",
        "        'weight_decay' : {\n",
        "            'values' : [0.0005]\n",
        "        },\n",
        "        'loss' : {\n",
        "            'values' : ['cross_entropy']\n",
        "        }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  #Taking pixels as inputs\n",
        "  x_train_input = x_train_norm.reshape(len(x_train_norm), num_of_pixels)                      #flattening the image into 1d array\n",
        "  x_test_input = x_test_norm.reshape(len(x_test_norm), num_of_pixels)                         #same thing\n",
        "  x_val_reshape = x_val_norm.reshape(len(x_val_norm), num_of_pixels)\n",
        "  x_train_input = x_train_input.T\n",
        "  x_test_input = x_test_input.T\n",
        "  x_val = x_val_reshape.T\n",
        "\n",
        "  data_size = len(x_train_input[0])\n",
        "\n",
        "  # nn = NeuralNetwork(num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons)\n",
        "\n",
        "  # weights, biases = nn.initialize_parameters(num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons, initialization)\n",
        "\n",
        "  # weights, biases = train_neural_network(nn, x_train_input, y_train, x_test_input, y_test, x_val, y_val, weights, biases, num_hidden_layers, activation_function, optimizer, epochs, batch_size, eta, beta, beta1, beta2, eps, weight_decay, loss)\n",
        "\n",
        "  class_names=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "  run_name = \"\"\n",
        "\n",
        "  def train():\n",
        "    with wandb.init() as run:\n",
        "      config = wandb.config\n",
        "      run_name = \"hl_\" + str(config.num_hidden_layers) + \"_bs_\" + str(config.batch_size) + \"_ac_\" + config.activation_function\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "      # printImages(x_train, y_train)\n",
        "      hidden_neurons_list = []\n",
        "      for i in range(config.num_hidden_layers):\n",
        "        hidden_neurons_list.append(config.num_hidden_neurons)\n",
        "      nn = NeuralNetwork(num_of_pixels, hidden_neurons_list, config.num_hidden_layers, output_neurons)\n",
        "      weights, biases = nn.initialize_parameters(num_of_pixels, hidden_neurons_list, config.num_hidden_layers, output_neurons, config.initialization)\n",
        "      weights, biases = train_neural_network(nn, x_train_input, y_train, x_test_input, y_test, x_val, y_val, weights, biases, config.num_hidden_layers, config.activation_function,\n",
        "                                             config.optimizer, config.epochs, config.batch_size, config.eta, beta, beta1, beta2, eps, config.weight_decay, config.loss)\n",
        "\n",
        "      # _, _, y_test_pred = nn.feedforward_propagation(x_test_input, weights, biases, config.num_hidden_layers, config.activation_function)\n",
        "      # y_test_pred = np.argmax(y_test_pred, axis=0)\n",
        "      # conf_matrix = wandb.plot.confusion_matrix(y_true = y_test, preds = y_test_pred, class_names = class_names)\n",
        "      # wandb.sklearn.plot_confusion_matrix(y_test, y_test_pred, class_names)\n",
        "  sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assignment_1')\n",
        "\n",
        "  wandb.agent(sweep_id, function=train,count=50)\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RADLWjLaj8Q0",
        "outputId": "8a5c26b8-6910-4e7c-8124-633c475f6882"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: o2r1u0l4\n",
            "Sweep URL: https://wandb.ai/cs23m024-gaurav/DL_Assignment_1/sweeps/o2r1u0l4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ucxmggzr with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teta: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: normal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    }
  ]
}