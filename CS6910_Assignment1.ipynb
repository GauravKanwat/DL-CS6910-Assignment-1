{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPU1dWziu+eC7AEYLiJWEBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravKanwat/CS6910_Assignment1/blob/main/CS6910_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlvKesVaMN4q",
        "outputId": "dba93ba0-d31c-4fc0-9d20-7470ff09c0c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "HD7cQlG1MEMc",
        "outputId": "79ca2736-6c80-4b25-84f5-355022be8143"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wandb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-aeec82056059>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "#    project=\"DL-Assignment-1\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "#    config={\n",
        "#    \"eta\": 0.02,\n",
        "#    \"architecture\": \"CNN\",\n",
        "#    \"dataset\": \"CIFAR-100\",\n",
        "#    \"epochs\": 10,\n",
        "#    }\n",
        "#)"
      ],
      "metadata": {
        "id": "kvVZomK5MqUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx4k0rtLztlX",
        "outputId": "ebd963a6-8fe1-46fa-e02b-67ea7d21054c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/51 [00:01<00:53,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0, Accuracy: 0.1019, Loss: 176.96867275650573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 11/51 [00:08<00:31,  1.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 10, Accuracy: 0.58074, Loss: 144.4914034596916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 21/51 [00:17<00:24,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 20, Accuracy: 0.69606, Loss: 164.89146359049042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 31/51 [00:26<00:20,  1.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 30, Accuracy: 0.72114, Loss: 179.8014131210799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 41/51 [00:34<00:07,  1.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 40, Accuracy: 0.73464, Loss: 190.7055024683113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:42<00:00,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 50, Accuracy: 0.74292, Loss: 199.20920827882202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, x_input, y_input, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "      self.x_input = x_input\n",
        "      self.y_input = y_input\n",
        "      self.num_of_pixels = num_of_pixels\n",
        "      self.hidden_neurons_list = hidden_neurons_list\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.output_neurons = output_neurons\n",
        "\n",
        "\n",
        "    def initialize_parameters(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        for l in range(num_hidden_layers):\n",
        "          weights[l] = np.random.rand(hidden_neurons_list[l], num_of_pixels if l == 0 else hidden_neurons_list[l-1]) - 0.5\n",
        "          biases[l] = np.random.rand(hidden_neurons_list[l], 1) - 0.5\n",
        "        weights[num_hidden_layers] = np.random.rand(output_neurons, hidden_neurons_list[-1]) - 0.5\n",
        "        biases[num_hidden_layers] = np.random.rand(output_neurons, 1) - 0.5\n",
        "        return weights, biases\n",
        "\n",
        "    def xavier_intialization(self, num_of_pixels, hidden_neurons_list, output_neurons):\n",
        "        num_layers = len(hidden_neurons_list) + 1\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "\n",
        "        weights[0] = np.random.randn(hidden_neurons_list[0], num_of_pixels) * np.sqrt(1 / num_of_pixels)\n",
        "        biases[0] = np.zeros((hidden_neurons_list[0], 1))\n",
        "\n",
        "        # Initialize weights and biases for hidden layers\n",
        "        for l in range(1, len(hidden_neurons_list)):\n",
        "            weights[l] = np.random.randn(hidden_neurons_list[l], hidden_neurons_list[l-1]) * np.sqrt(1 / hidden_neurons_list[l-1])\n",
        "            biases[l] = np.zeros((hidden_neurons_list[l], 1))\n",
        "\n",
        "        # Initialize weights for last hidden layer to output layer\n",
        "        weights[len(hidden_neurons_list)] = np.random.randn(output_neurons, hidden_neurons_list[-1]) * np.sqrt(1 / hidden_neurons_list[-1])\n",
        "        biases[len(hidden_neurons_list)] = np.zeros((output_neurons, 1))\n",
        "        return weights, biases\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        #return 1 / (1 + np.exp(-x))\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    def reLU(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_x = np.max(x, axis=0)\n",
        "        exp_x = np.exp(x - max_x)  # avoiding overflow\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "    def feedforward_propagation(self, X, weights, biases, num_hidden_layers, activation_function):\n",
        "        a = {}\n",
        "        h = {}\n",
        "\n",
        "        for k in range(num_hidden_layers):\n",
        "            if k == 0:\n",
        "              a[k] = np.dot(weights[k], X) + biases[k]\n",
        "              if(activation_function == \"reLU\"):\n",
        "                h[k] = self.reLU(a[k])\n",
        "              elif(activation_function == \"sigmoid\"):\n",
        "                h[k] = self.sigmoid(a[k])\n",
        "              elif(activation_function == \"tanh\"):\n",
        "                h[k] = self.tanh(a[k])\n",
        "            else:\n",
        "              a[k] = np.dot(weights[k], h[k-1]) + biases[k]\n",
        "              if(activation_function == \"reLU\"):\n",
        "                h[k] = self.reLU(a[k])\n",
        "              elif(activation_function == \"sigmoid\"):\n",
        "                h[k] = self.sigmoid(a[k])\n",
        "              elif(activation_function == \"tanh\"):\n",
        "                h[k] = self.tanh(a[k])\n",
        "\n",
        "        a[num_hidden_layers] = np.dot(weights[num_hidden_layers], h[num_hidden_layers - 1]) + biases[num_hidden_layers]\n",
        "        y_hat = self.softmax(a[num_hidden_layers])\n",
        "        return a, h, y_hat\n",
        "\n",
        "    def one_hot(self, Y):\n",
        "      if Y.max() != 9:\n",
        "        one_hot_Y = np.zeros((Y.size, 10))\n",
        "      else:\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "      one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "      one_hot_Y = one_hot_Y.T\n",
        "      return one_hot_Y\n",
        "\n",
        "    def deriv_sigmoid(self, Z):\n",
        "      func = self.sigmoid(Z)\n",
        "      return func * (1 - func)\n",
        "\n",
        "    def deriv_reLU(self, Z):\n",
        "      return Z > 0\n",
        "\n",
        "    def deriv_tanh(self, x):\n",
        "      sechX = 1 / np.cosh(x)\n",
        "      return sechX ** 2\n",
        "\n",
        "    def back_propagation(self, Y, fwd_A, fwd_H, weights, biases, pred_output, num_hidden_layers, activation_function):\n",
        "      one_hot_Y = self.one_hot(Y)\n",
        "      dA = {}\n",
        "      dH = {}\n",
        "      dW = {}\n",
        "      dB = {}\n",
        "\n",
        "      dA[num_hidden_layers] = pred_output - one_hot_Y\n",
        "\n",
        "      for k in range(num_hidden_layers, 0, -1):\n",
        "        dW[k] = np.dot(dA[k], fwd_H[k-1].T)\n",
        "        dB[k] = np.mean(dA[k], axis=1, keepdims=True)\n",
        "\n",
        "        dH[k-1] = np.dot(weights[k].T, dA[k])\n",
        "        if(activation_function == \"reLU\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_reLU(fwd_A[k-1]))\n",
        "        elif(activation_function == \"sigmoid\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_sigmoid(fwd_A[k-1]))\n",
        "        elif(activation_function == \"tanh\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_tanh(fwd_A[k-1]))\n",
        "      return dW, dB\n",
        "\n",
        "    def get_predictions(self, pred_output):\n",
        "      return np.argmax(pred_output, axis = 0)\n",
        "\n",
        "    def get_accuracy(self, y_pred, y_true):\n",
        "      return np.sum(y_pred == y_true) / y_true.size\n",
        "\n",
        "    def cross_entropy(self, y_pred, y_true):\n",
        "      epsilon = 1e-15\n",
        "      loss = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=0))\n",
        "      return loss\n",
        "\n",
        "    def gradient_descent(self, epochs, eta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        total_loss = 0\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        loss = self.cross_entropy(pred_output, self.y_input)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          weights[l] -= eta * del_w[l]\n",
        "          biases[l] -= eta * del_b[l]\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}, Loss: {total_loss}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def stochastic_gradient_descent(self, epochs, eta, activation_function, initialization, batch_size):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      num_samples = self.x_input.shape[1]\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        total_loss = 0\n",
        "        shuffled_indices = np.random.permutation(num_samples)\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "\n",
        "          batch_indices = shuffled_indices[i:i+batch_size]\n",
        "          x_batch = self.x_input[:, batch_indices]\n",
        "          y_batch = self.y_input[batch_indices]\n",
        "\n",
        "          fwd_a, fwd_h, pred_output = self.feedforward_propagation(x_batch, weights, biases, self.num_hidden_layers, activation_function)\n",
        "          del_w, del_b = self.back_propagation(y_batch, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "          loss = self.cross_entropy(pred_output, self.y_input)\n",
        "\n",
        "          # Update weights and biases\n",
        "          for l in range(1, self.num_hidden_layers + 1):\n",
        "            weights[l] -= eta * del_w[l]\n",
        "            biases[l] -= eta * del_b[l]\n",
        "\n",
        "        total_loss += loss\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), y_batch)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}, Loss: {total_loss}\")\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def momentum_based_gradient_descent(self, epochs, eta, beta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      prev_uw = {}\n",
        "      prev_ub = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        prev_uw[l] = 0\n",
        "        prev_ub[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        total_loss = 0\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        loss = self.cross_entropy(pred_output, self.y_input)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          uw = beta * prev_uw[l] + eta * del_w[l]\n",
        "          ub = beta * prev_ub[l] + eta * del_b[l]\n",
        "          weights[l] -= uw\n",
        "          biases[l] -= ub\n",
        "          prev_uw[l] = uw\n",
        "          prev_ub[l] = ub\n",
        "\n",
        "          total_loss = loss\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}, Loss: {total_loss}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, epochs, eta, beta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      prev_vw = 0\n",
        "      prev_vb = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        v_w = beta*prev_vw\n",
        "        v_b = beta*prev_vb\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          vw = beta * prev_vw + eta * del_w[l]\n",
        "          vb = beta * prev_vb + eta * del_b[l]\n",
        "          weights[l] -= vw\n",
        "          biases[l] -= vb\n",
        "          prev_uw = vw\n",
        "          prev_ub = vb\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def adagrad_gradient_descent(self, epochs, eta, eps, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          v_w[l] = v_w[l] + del_w[l]**2\n",
        "          v_b[l] = v_b[l] + del_b[l]**2\n",
        "\n",
        "          weights[l] -= eta * del_w[l] / (np.sqrt(v_w[l]) + eps)\n",
        "          biases[l] -= eta * del_b[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def rmsProp_gradient_descent(self, epochs, eta, eps, beta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          v_w[l] = (beta * v_w[l]) + ((1-beta) * del_w[l] ** 2)\n",
        "          v_b[l] = (beta * v_b[l]) + ((1-beta) * del_b[l] ** 2)\n",
        "\n",
        "          weights[l] -= eta * del_w[l] / (np.sqrt(v_w[l]) + eps)\n",
        "          biases[l] -= eta * del_b[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def adam_gradient_descent(self, epochs, eta, eps, beta1, beta2, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      print(self.x_input.shape)\n",
        "\n",
        "      m_w = {}\n",
        "      m_b = {}\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "      m_w_hat = {}\n",
        "      m_b_hat = {}\n",
        "      v_w_hat = {}\n",
        "      v_b_hat = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = 0\n",
        "        m_b[l] = 0\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "        m_w_hat[l] = 0\n",
        "        m_b_hat[l] = 0\n",
        "        v_w_hat[l] = 0\n",
        "        v_b_hat[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          m_w[l] = (beta1 * m_w[l]) + (1-beta1) * del_w[l]\n",
        "          m_b[l] = (beta1 * m_b[l]) + (1-beta1) * del_b[l]\n",
        "\n",
        "          v_w[l] = beta2 * v_w[l] + (1 - beta2) * (del_w[l] ** 2)\n",
        "          v_b[l] = beta2 * v_b[l] + (1 - beta2) * (del_b[l] ** 2)\n",
        "\n",
        "          m_w_hat[l] = m_w[l]/(1-np.power(beta1, l+1))\n",
        "          m_b_hat[l] = m_b[l]/(1-np.power(beta1, l+1))\n",
        "          v_w_hat[l] = v_w[l]/(1-np.power(beta2, l+1))\n",
        "          v_b_hat[l] = v_b[l]/(1-np.power(beta2, l+1))\n",
        "\n",
        "          #update parameters\n",
        "          weights[l] -= eta*m_w_hat[l]/(np.sqrt(v_w_hat[l])+eps)\n",
        "          biases[l] -= eta*m_b_hat[l]/(np.sqrt(v_b_hat[l])+eps)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  fashion_mnist = keras.datasets.fashion_mnist\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=10000, random_state=42)\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  x_train_norm = x_train / 255\n",
        "  x_test_norm = x_test / 255\n",
        "\n",
        "  # Define hyperparameters\n",
        "  num_of_pixels = 28 * 28                                                         #28 * 28 = 784 pixels\n",
        "  hidden_neurons_list = [128]\n",
        "  num_hidden_layers = len(hidden_neurons_list)\n",
        "  output_neurons = 10\n",
        "  eta = 1e-5\n",
        "  epochs = 51\n",
        "  activation_function = \"sigmoid\"\n",
        "  initialization = \"normal\"\n",
        "  batch_size = 1\n",
        "  beta = 0.5\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-10\n",
        "\n",
        "  #Taking pixels as inputs\n",
        "  x_train_input = x_train_norm.reshape(len(x_train_norm), num_of_pixels)                      #flattening the image into 1d array\n",
        "  x_test_input = x_test_norm.reshape(len(x_test_norm), num_of_pixels)                         #same thing\n",
        "  x_train_input = x_train_input.T\n",
        "  x_test_input = x_test_input.T\n",
        "\n",
        "  # Neural network class -> nn object\n",
        "  nn = NeuralNetwork(x_train_input, y_train, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons)\n",
        "\n",
        "  # Call the gradient_descent method\n",
        "  #weights, biases = nn.gradient_descent(epochs, eta, activation_function, initialization)\n",
        "  #weights, biases = nn.stochastic_gradient_descent(epochs, eta, activation_function, initialization, batch_size)\n",
        "  weights, biases = nn.momentum_based_gradient_descent(epochs, eta, beta, activation_function, initialization)\n",
        "  #weights, biases = nn.nesterov_accelerated_gradient_descent(epochs, eta, beta, activation_function, initialization)\n",
        "  #weights, biases = nn.adagrad_gradient_descent(epochs, eta, eps, activation_function, initialization)\n",
        "  #weights, biases = nn.rmsProp_gradient_descent(epochs, eta, eps, beta, activation_function, initialization)\n",
        "  #weights, biases = nn.adam_gradient_descent(epochs, eta, eps, beta1, beta2, activation_function, initialization)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}