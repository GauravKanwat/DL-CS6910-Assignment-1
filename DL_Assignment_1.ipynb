{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVcrTNeLFmX7xNmG3AioWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravKanwat/CS6910_Assignment1/blob/main/DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ve3xt74Bf59t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "      self.num_of_pixels = num_of_pixels\n",
        "      self.hidden_neurons_list = hidden_neurons_list\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.output_neurons = output_neurons\n",
        "\n",
        "\n",
        "    def initialize_parameters(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons, initialization):\n",
        "      '''\n",
        "          Initializing the weights and biases, both are dictionary which are storing random values generated by rand between (0 to 1) and subtracting 0.5 from it makes it between\n",
        "          -0.5 to 0.5\n",
        "      '''\n",
        "\n",
        "      weights = {}\n",
        "      biases = {}\n",
        "      num_layers = len(hidden_neurons_list) + 1\n",
        "\n",
        "      if initialization == \"xavier\":\n",
        "        weights[0] = np.random.randn(hidden_neurons_list[0], num_of_pixels) * np.sqrt(1 / num_of_pixels)\n",
        "        biases[0] = np.zeros((hidden_neurons_list[0], 1))\n",
        "\n",
        "        # Initialize weights and biases for hidden layers\n",
        "        for l in range(1, len(hidden_neurons_list)):\n",
        "          weights[l] = np.random.randn(hidden_neurons_list[l], hidden_neurons_list[l-1]) * np.sqrt(1 / hidden_neurons_list[l-1])\n",
        "          biases[l] = np.zeros((hidden_neurons_list[l], 1))\n",
        "\n",
        "        # Initialize weights for last hidden layer to output layer\n",
        "        weights[len(hidden_neurons_list)] = np.random.randn(output_neurons, hidden_neurons_list[-1]) * np.sqrt(1 / hidden_neurons_list[-1])\n",
        "        biases[len(hidden_neurons_list)] = np.zeros((output_neurons, 1))\n",
        "        return weights, biases\n",
        "\n",
        "      else:\n",
        "        weights[0] = np.random.rand(hidden_neurons_list[0], num_of_pixels) - 0.5\n",
        "        biases[0] = np.random.rand(hidden_neurons_list[0], 1) - 0.5\n",
        "        for l in range(num_hidden_layers):\n",
        "          weights[l] = np.random.rand(hidden_neurons_list[l], num_of_pixels if l == 0 else hidden_neurons_list[l-1]) - 0.5\n",
        "          biases[l] = np.random.rand(hidden_neurons_list[l], 1) - 0.5\n",
        "        weights[num_hidden_layers] = np.random.rand(output_neurons, hidden_neurons_list[-1]) - 0.5\n",
        "        biases[num_hidden_layers] = np.random.rand(output_neurons, 1) - 0.5\n",
        "      return weights, biases\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "      sigmoid_x = np.where(x < -30, 1, 1 / (1 + np.exp(-x)))\n",
        "      return sigmoid_x\n",
        "\n",
        "    def reLU(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_x = np.max(x, axis=0)\n",
        "        exp_x = np.exp(x - max_x)  # avoiding overflow\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "    def feedforward_propagation(self, X, weights, biases, num_hidden_layers, activation_function):\n",
        "      a = []\n",
        "      h = []\n",
        "\n",
        "      for k in range(num_hidden_layers):\n",
        "          if k == 0:\n",
        "            a.append(np.dot(weights[k], X) + biases[k])\n",
        "            if(activation_function == \"reLU\"):\n",
        "              h.append(self.reLU(a[k]))\n",
        "            elif(activation_function == \"sigmoid\"):\n",
        "              h.append(self.sigmoid(a[k]))\n",
        "            elif(activation_function == \"tanh\"):\n",
        "              h.append(self.tanh(a[k]))\n",
        "          else:\n",
        "            a.append(np.dot(weights[k], h[k-1]) + biases[k])\n",
        "            if(activation_function == \"reLU\"):\n",
        "              h.append(self.reLU(a[k]))\n",
        "            elif(activation_function == \"sigmoid\"):\n",
        "              h.append(self.sigmoid(a[k]))\n",
        "            elif(activation_function == \"tanh\"):\n",
        "              h.append(self.tanh(a[k]))\n",
        "\n",
        "      a.append(np.dot(weights[num_hidden_layers], h[num_hidden_layers - 1]) + biases[num_hidden_layers])\n",
        "      y_hat = self.softmax(a[-1])\n",
        "      return a, h, y_hat\n",
        "\n",
        "    def one_hot(self, Y):\n",
        "      if Y.max() != 9:\n",
        "        one_hot_Y = np.zeros((Y.size, 10))\n",
        "      else:\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "      one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "      one_hot_Y = one_hot_Y.T\n",
        "      return one_hot_Y\n",
        "\n",
        "    def deriv_sigmoid(self, Z):\n",
        "      func = self.sigmoid(Z)\n",
        "      return func * (1 - func)\n",
        "\n",
        "    def deriv_reLU(self, Z):\n",
        "      return Z > 0\n",
        "\n",
        "    def deriv_tanh(self, x):\n",
        "      return 1 - np.tanh(x)**2\n",
        "      # sechX = 1 / np.cosh(x)\n",
        "      # return sechX ** 2\n",
        "\n",
        "    def back_propagation(self, Y, fwd_A, fwd_H, weights, biases, pred_output, num_hidden_layers, activation_function):\n",
        "      one_hot_Y = self.one_hot(Y)\n",
        "      dA = {}\n",
        "      dH = {}\n",
        "      dW = {}\n",
        "      dB = {}\n",
        "\n",
        "      dA[num_hidden_layers] = pred_output - one_hot_Y\n",
        "\n",
        "      for k in range(num_hidden_layers, 0, -1):\n",
        "        dW[k] = np.dot(dA[k], fwd_H[k-1].T)\n",
        "        dB[k] = np.mean(dA[k], axis=1, keepdims=True)\n",
        "\n",
        "        dH[k-1] = np.dot(weights[k].T, dA[k])\n",
        "        if(activation_function == \"reLU\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_reLU(fwd_A[k-1]))\n",
        "        elif(activation_function == \"sigmoid\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_sigmoid(fwd_A[k-1]))\n",
        "        elif(activation_function == \"tanh\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_tanh(fwd_A[k-1]))\n",
        "      return dW, dB\n",
        "\n",
        "    def get_predictions(self, pred_output):\n",
        "      return np.argmax(pred_output, axis = 0)\n",
        "\n",
        "    def get_accuracy(self, y_pred, y_true):\n",
        "      return np.sum(y_pred == y_true) / y_true.size\n",
        "\n",
        "    def cross_entropy(self, y_pred, y_true):\n",
        "     epsilon = 1e-15\n",
        "     loss = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=0))\n",
        "     return loss\n",
        "\n",
        "    def gradient_descent(self, weights, biases, dW, dB, eta):\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        weights[l] -= eta * dW[l]\n",
        "        biases[l] -= eta * dB[l]\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def momentum_based_gradient_descent(self, weights, biases, dW, dB, epochs, eta, beta):\n",
        "      prev_uw = {}\n",
        "      prev_ub = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        prev_uw[l] = 0\n",
        "        prev_ub[l] = 0\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        uw = beta * prev_uw[l] + eta * dW[l]\n",
        "        ub = beta * prev_ub[l] + eta * dB[l]\n",
        "        weights[l] -= uw\n",
        "        biases[l] -= ub\n",
        "        prev_uw[l] = uw\n",
        "        prev_ub[l] = ub\n",
        "      return weights, biases\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, weights, biases, dW, dB, epochs, eta, beta):\n",
        "      prev_vw = 0\n",
        "      prev_vb = 0\n",
        "      v_w = beta*prev_vw\n",
        "      v_b = beta*prev_vb\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        vw = beta * prev_vw + eta * dW[l]\n",
        "        vb = beta * prev_vb + eta * dB[l]\n",
        "        weights[l] -= vw\n",
        "        biases[l] -= vb\n",
        "        prev_uw = vw\n",
        "        prev_ub = vb\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def adagrad_gradient_descent(self, weights, biases, dW, dB, eta, eps):\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = v_w[l] + dW[l]**2\n",
        "        v_b[l] = v_b[l] + dB[l]**2\n",
        "        weights[l] -= eta * dW[l] / (np.sqrt(v_w[l]) + eps)\n",
        "        biases[l] -= eta * dB[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def rmsProp_gradient_descent(self, weights, biases, dW, dB, epochs, eta, eps, beta):\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = (beta * v_w[l]) + ((1-beta) * dW[l] ** 2)\n",
        "        v_b[l] = (beta * v_b[l]) + ((1-beta) * dB[l] ** 2)\n",
        "\n",
        "        weights[l] -= eta * dW[l] / (np.sqrt(v_w[l]) + eps)\n",
        "        biases[l] -= eta * dB[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def adam_gradient_descent(self, weights, biases, dW, dB, epochs, eta, eps, beta1, beta2):\n",
        "      m_w = {}\n",
        "      m_b = {}\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "      m_w_hat = {}\n",
        "      m_b_hat = {}\n",
        "      v_w_hat = {}\n",
        "      v_b_hat = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = 0\n",
        "        m_b[l] = 0\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "        m_w_hat[l] = 0\n",
        "        m_b_hat[l] = 0\n",
        "        v_w_hat[l] = 0\n",
        "        v_b_hat[l] = 0\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = (beta1 * m_w[l]) + (1-beta1) * dW[l]\n",
        "        m_b[l] = (beta1 * m_b[l]) + (1-beta1) * dB[l]\n",
        "\n",
        "        v_w[l] = beta2 * v_w[l] + (1 - beta2) * (dW[l] ** 2)\n",
        "        v_b[l] = beta2 * v_b[l] + (1 - beta2) * (dB[l] ** 2)\n",
        "\n",
        "        m_w_hat[l] = m_w[l]/(1-np.power(beta1, l+1))\n",
        "        m_b_hat[l] = m_b[l]/(1-np.power(beta1, l+1))\n",
        "        v_w_hat[l] = v_w[l]/(1-np.power(beta2, l+1))\n",
        "        v_b_hat[l] = v_b[l]/(1-np.power(beta2, l+1))\n",
        "\n",
        "        #update parameters\n",
        "        weights[l] -= eta*m_w_hat[l]/(np.sqrt(v_w_hat[l])+eps)\n",
        "        biases[l] -= eta*m_b_hat[l]/(np.sqrt(v_b_hat[l])+eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "    def nadam_gradient_descent(self, weights, biases, dW, dB, epochs, eta, eps, beta1, beta2):\n",
        "      m_w = {}\n",
        "      m_b = {}\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "      m_w_hat = {}\n",
        "      m_b_hat = {}\n",
        "      v_w_hat = {}\n",
        "      v_b_hat = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = 0\n",
        "        m_b[l] = 0\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "        m_w_hat[l] = 0\n",
        "        m_b_hat[l] = 0\n",
        "        v_w_hat[l] = 0\n",
        "        v_b_hat[l] = 0\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = (beta1 * m_w[l]) + (1-beta1) * dW[l]\n",
        "        m_b[l] = (beta1 * m_b[l]) + (1-beta1) * dB[l]\n",
        "\n",
        "        v_w[l] = beta2 * v_w[l] + (1 - beta2) * (dW[l] ** 2)\n",
        "        v_b[l] = beta2 * v_b[l] + (1 - beta2) * (dB[l] ** 2)\n",
        "\n",
        "        m_w_hat[l] = m_w[l]/(1-np.power(beta1, l+1))\n",
        "        m_b_hat[l] = m_b[l]/(1-np.power(beta1, l+1))\n",
        "        v_w_hat[l] = v_w[l]/(1-np.power(beta2, l+1))\n",
        "        v_b_hat[l] = v_b[l]/(1-np.power(beta2, l+1))\n",
        "\n",
        "        #update parameters\n",
        "        weights[l] -= (eta/np.sqrt(v_w_hat[l] + eps)) * (beta1 * m_w_hat[l] + (1-beta1) * dW[l] / (1-np.power(beta1, l+1)))\n",
        "        biases[l] -= (eta/np.sqrt(v_b_hat[l] + eps)) * (beta1 * m_b_hat[l] + (1-beta1) * dB[l] / (1-np.power(beta1, l+1)))\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def compute_accuracy(self, X_test, y_test, weights, biases, num_hidden_layers, activation_function):\n",
        "\n",
        "      _, _, pred_output = self.feedforward_propagation(X_test, weights, biases, num_hidden_layers, activation_function)\n",
        "      pred_labels = np.argmax(pred_output, axis=0)\n",
        "      accuracy = np.mean(pred_labels == y_test)\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "mSOxH8YYgED4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  fashion_mnist = keras.datasets.fashion_mnist\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=10000, random_state=42)\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  x_train_norm = x_train / 255\n",
        "  x_test_norm = x_test / 255\n",
        "  x_val_norm = x_val / 255\n",
        "\n",
        "  # Define hyperparameters\n",
        "  num_of_pixels = 28 * 28                                                         #28 * 28 = 784 pixels\n",
        "  num_hidden_layers = 3\n",
        "  num_hidden_neurons = 128\n",
        "  hidden_neurons_list = []\n",
        "  for i in range(num_hidden_layers):\n",
        "    hidden_neurons_list.append(num_hidden_neurons)\n",
        "  output_neurons = 10\n",
        "  eta = 1e-3\n",
        "  epochs = 10\n",
        "  activation_function = \"tanh\"\n",
        "  initialization = \"normal\"\n",
        "  opt_function = \"simple\"\n",
        "  batch_size = 32\n",
        "  beta = 0.5\n",
        "  beta1 = 0.9\n",
        "  beta2 = 0.999\n",
        "  eps = 1e-8\n",
        "\n",
        "  #Taking pixels as inputs\n",
        "  x_train_input = x_train_norm.reshape(len(x_train_norm), num_of_pixels)                      #flattening the image into 1d array\n",
        "  x_test_input = x_test_norm.reshape(len(x_test_norm), num_of_pixels)                         #same thing\n",
        "  x_val_reshape = x_val_norm.reshape(len(x_val_norm), num_of_pixels)\n",
        "  x_train_input = x_train_input.T\n",
        "  x_test_input = x_test_input.T\n",
        "  x_val = x_val_reshape.T\n",
        "\n",
        "  data_size = len(x_train_input[0])\n",
        "  #print(x_train_input.shape)\n",
        "  #print(data_size)\n",
        "\n",
        "\n",
        "  nn = NeuralNetwork(num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons)\n",
        "\n",
        "  weights, biases = nn.initialize_parameters(num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons, initialization)\n",
        "\n",
        "  for iter in tqdm(range(epochs)):\n",
        "    for i in range(0, data_size, batch_size):\n",
        "      if i<= data_size - batch_size:\n",
        "        X_batch = x_train_input[:, i:i+batch_size]\n",
        "        Y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "        if opt_function == \"simple\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.gradient_descent(weights, biases, dW, dB, eta)\n",
        "\n",
        "        elif opt_function == \"momentum\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.momentum_based_gradient_descent(weights, biases, dW, dB, epochs, eta, beta)\n",
        "\n",
        "        elif opt_function == \"nesterov\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.nesterov_accelerated_gradient_descent(weights, biases, dW, dB, epochs, eta, beta)\n",
        "        elif opt_function == \"rmsProp\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.rmsProp_gradient_descent(weights, biases, dW, dB, epochs, eta, eps, beta)\n",
        "        elif opt_function == \"adam\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.adam_gradient_descent(weights, biases, dW, dB, epochs, eta, eps, beta1, beta2)\n",
        "        elif opt_function == \"nadam\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.nadam_gradient_descent(weights, biases, dW, dB, epochs, eta, eps, beta1, beta2)\n",
        "\n",
        "    accuracy = nn.compute_accuracy(x_val, y_val, weights, biases, num_hidden_layers, activation_function)\n",
        "    print(f\"Accuracy on Validation set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkJl4tQAgp0z",
        "outputId": "56640c36-d959-4967-fd5b-9b82b1f0fac2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:07<01:05,  7.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 77.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:12<00:49,  6.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 79.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:19<00:45,  6.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 80.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [00:25<00:36,  6.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 80.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [00:32<00:32,  6.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 81.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [00:37<00:24,  6.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 81.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [00:47<00:22,  7.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 81.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [00:55<00:15,  7.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 82.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [01:00<00:06,  6.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 82.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:07<00:00,  6.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Validation set: 82.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}