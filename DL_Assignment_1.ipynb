{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR+xY65Qzq3x03XGuXC7HB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravKanwat/DL_Assignment_1/blob/main/DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx4k0rtLztlX",
        "outputId": "97af7d1d-1b04-4f94-caf7-8d173ac829ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/51 [00:00<00:12,  3.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0, Accuracy: 0.1044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 11/51 [00:03<00:14,  2.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 10, Accuracy: 0.5163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 21/51 [00:06<00:09,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 20, Accuracy: 0.6134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████    | 31/51 [00:09<00:05,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 30, Accuracy: 0.7036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 41/51 [00:12<00:02,  3.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 40, Accuracy: 0.7022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:14<00:00,  3.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 50, Accuracy: 0.723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x_input, y_input, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "      self.x_input = x_input\n",
        "      self.y_input = y_input\n",
        "      self.num_of_pixels = num_of_pixels\n",
        "      self.hidden_neurons_list = hidden_neurons_list\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.output_neurons = output_neurons\n",
        "\n",
        "\n",
        "    def initialize_parameters(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        for l in range(num_hidden_layers):\n",
        "          weights[l] = np.random.rand(hidden_neurons_list[l], num_of_pixels if l == 0 else hidden_neurons_list[l-1]) - 0.5\n",
        "          biases[l] = np.random.rand(hidden_neurons_list[l], 1) - 0.5\n",
        "        weights[num_hidden_layers] = np.random.rand(output_neurons, hidden_neurons_list[-1]) - 0.5\n",
        "        biases[num_hidden_layers] = np.random.rand(output_neurons, 1) - 0.5\n",
        "        return weights, biases\n",
        "\n",
        "    def xavier_intialization(self, num_of_pixels, hidden_neurons_list, output_neurons):\n",
        "        num_layers = len(hidden_neurons_list) + 1\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "\n",
        "        weights[0] = np.random.randn(hidden_neurons_list[0], num_of_pixels) * np.sqrt(1 / num_of_pixels)\n",
        "        biases[0] = np.zeros((hidden_neurons_list[0], 1))\n",
        "\n",
        "        # Initialize weights and biases for hidden layers\n",
        "        for l in range(1, len(hidden_neurons_list)):\n",
        "            weights[l] = np.random.randn(hidden_neurons_list[l], hidden_neurons_list[l-1]) * np.sqrt(1 / hidden_neurons_list[l-1])\n",
        "            biases[l] = np.zeros((hidden_neurons_list[l], 1))\n",
        "\n",
        "        # Initialize weights for last hidden layer to output layer\n",
        "        weights[len(hidden_neurons_list)] = np.random.randn(output_neurons, hidden_neurons_list[-1]) * np.sqrt(1 / hidden_neurons_list[-1])\n",
        "        biases[len(hidden_neurons_list)] = np.zeros((output_neurons, 1))\n",
        "        return weights, biases\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        #return 1 / (1 + np.exp(-x))\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    def reLU(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_x = np.max(x, axis=0)\n",
        "        exp_x = np.exp(x - max_x)  # avoiding overflow\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "    def feedforward_propagation(self, X, weights, biases, num_hidden_layers, activation_function):\n",
        "        a = {}\n",
        "        h = {}\n",
        "\n",
        "        for k in range(num_hidden_layers):\n",
        "            if k == 0:\n",
        "              a[k] = np.dot(weights[k], X) + biases[k]\n",
        "              if(activation_function == \"reLU\"):\n",
        "                h[k] = self.reLU(a[k])\n",
        "              elif(activation_function == \"sigmoid\"):\n",
        "                h[k] = self.sigmoid(a[k])\n",
        "              elif(activation_function == \"tanh\"):\n",
        "                h[k] = self.tanh(a[k])\n",
        "            else:\n",
        "              a[k] = np.dot(weights[k], h[k-1]) + biases[k]\n",
        "              if(activation_function == \"reLU\"):\n",
        "                h[k] = self.reLU(a[k])\n",
        "              elif(activation_function == \"sigmoid\"):\n",
        "                h[k] = self.sigmoid(a[k])\n",
        "              elif(activation_function == \"tanh\"):\n",
        "                h[k] = self.tanh(a[k])\n",
        "\n",
        "        a[num_hidden_layers] = np.dot(weights[num_hidden_layers], h[num_hidden_layers - 1]) + biases[num_hidden_layers]\n",
        "        y_hat = self.softmax(a[num_hidden_layers])\n",
        "        return a, h, y_hat\n",
        "\n",
        "    def one_hot(self, Y):\n",
        "      if Y.max() != 9:\n",
        "        one_hot_Y = np.zeros((Y.size, 10))\n",
        "      else:\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "      one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "      one_hot_Y = one_hot_Y.T\n",
        "      return one_hot_Y\n",
        "\n",
        "    def deriv_sigmoid(self, Z):\n",
        "      func = self.sigmoid(Z)\n",
        "      return func * (1 - func)\n",
        "\n",
        "    def deriv_reLU(self, Z):\n",
        "      return Z > 0\n",
        "\n",
        "    def deriv_tanh(self, x):\n",
        "      sechX = 1 / np.cosh(x)\n",
        "      return sechX ** 2\n",
        "\n",
        "    def back_propagation(self, Y, fwd_A, fwd_H, weights, biases, pred_output, num_hidden_layers, activation_function):\n",
        "      one_hot_Y = self.one_hot(Y)\n",
        "      dA = {}\n",
        "      dH = {}\n",
        "      dW = {}\n",
        "      dB = {}\n",
        "\n",
        "      dA[num_hidden_layers] = pred_output - one_hot_Y\n",
        "\n",
        "      for k in range(num_hidden_layers, 0, -1):\n",
        "        dW[k] = np.dot(dA[k], fwd_H[k-1].T)\n",
        "        dB[k] = np.mean(dA[k], axis=1, keepdims=True)\n",
        "\n",
        "        dH[k-1] = np.dot(weights[k].T, dA[k])\n",
        "        if(activation_function == \"reLU\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_reLU(fwd_A[k-1]))\n",
        "        elif(activation_function == \"sigmoid\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_sigmoid(fwd_A[k-1]))\n",
        "        elif(activation_function == \"tanh\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_tanh(fwd_A[k-1]))\n",
        "      return dW, dB\n",
        "\n",
        "    def get_predictions(self, pred_output):\n",
        "      return np.argmax(pred_output, axis = 0)\n",
        "\n",
        "    def get_accuracy(self, y_pred, y_true):\n",
        "      return np.sum(y_pred == y_true) / y_true.size\n",
        "\n",
        "    def gradient_descent(self, epochs, eta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          weights[l] -= eta * del_w[l]\n",
        "          biases[l] -= eta * del_b[l]\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def stochastic_gradient_descent(self, epochs, eta, activation_function, initialization, batch_size):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      num_samples = self.x_input.shape[1]\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        shuffled_indices = np.random.permutation(num_samples)\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "\n",
        "          batch_indices = shuffled_indices[i:i+batch_size]\n",
        "          x_batch = self.x_input[:, batch_indices]\n",
        "          y_batch = self.y_input[batch_indices]\n",
        "\n",
        "          fwd_a, fwd_h, pred_output = self.feedforward_propagation(x_batch, weights, biases, self.num_hidden_layers, activation_function)\n",
        "          del_w, del_b = self.back_propagation(y_batch, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "          # Update weights and biases\n",
        "          for l in range(1, self.num_hidden_layers + 1):\n",
        "            weights[l] -= eta * del_w[l]\n",
        "            biases[l] -= eta * del_b[l]\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), y_batch)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def momentum_based_gradient_descent(self, epochs, eta, beta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      prev_uw = {}\n",
        "      prev_ub = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        prev_uw[l] = 0\n",
        "        prev_ub[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          uw = beta * prev_uw[l] + eta * del_w[l]\n",
        "          ub = beta * prev_ub[l] + eta * del_b[l]\n",
        "          weights[l] -= uw\n",
        "          biases[l] -= ub\n",
        "          prev_uw[l] = uw\n",
        "          prev_ub[l] = ub\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, epochs, eta, beta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      prev_vw = 0\n",
        "      prev_vb = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        v_w = beta*prev_vw\n",
        "        v_b = beta*prev_vb\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          vw = beta * prev_vw + eta * del_w[l]\n",
        "          vb = beta * prev_vb + eta * del_b[l]\n",
        "          weights[l] -= vw\n",
        "          biases[l] -= vb\n",
        "          prev_uw = vw\n",
        "          prev_ub = vb\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def adagrad_gradient_descent(self, epochs, eta, eps, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          v_w[l] = v_w[l] + del_w[l]**2\n",
        "          v_b[l] = v_b[l] + del_b[l]**2\n",
        "\n",
        "          weights[l] -= eta * del_w[l] / (np.sqrt(v_w[l]) + eps)\n",
        "          biases[l] -= eta * del_b[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def rmsProp_gradient_descent(self, epochs, eta, eps, beta, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          v_w[l] = (beta * v_w[l]) + ((1-beta) * del_w[l] ** 2)\n",
        "          v_b[l] = (beta * v_b[l]) + ((1-beta) * del_b[l] ** 2)\n",
        "\n",
        "          weights[l] -= eta * del_w[l] / (np.sqrt(v_w[l]) + eps)\n",
        "          biases[l] -= eta * del_b[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "    def adam_gradient_descent(self, epochs, eta, eps, beta1, beta2, activation_function, initialization):\n",
        "      if(initialization == \"normal\"):\n",
        "        weights, biases = self.initialize_parameters(self.num_of_pixels, self.hidden_neurons_list, self.num_hidden_layers, self.output_neurons)\n",
        "      elif(initialization == \"xavier\"):\n",
        "        weights, biases = self.xavier_intialization(self.num_of_pixels, self.hidden_neurons_list, self.output_neurons)\n",
        "\n",
        "      m_w = {}\n",
        "      m_b = {}\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "      m_w_hat = {}\n",
        "      m_b_hat = {}\n",
        "      v_w_hat = {}\n",
        "      v_b_hat = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        m_w[l] = 0\n",
        "        m_b[l] = 0\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "        m_w_hat[l] = 0\n",
        "        m_b_hat[l] = 0\n",
        "        v_w_hat[l] = 0\n",
        "        v_b_hat[l] = 0\n",
        "\n",
        "      for epoch in tqdm(range(epochs)):\n",
        "        fwd_a, fwd_h, pred_output = self.feedforward_propagation(self.x_input, weights, biases, self.num_hidden_layers, activation_function)\n",
        "        del_w, del_b = self.back_propagation(self.y_input, fwd_a, fwd_h, weights, biases, pred_output, self.num_hidden_layers, activation_function)\n",
        "\n",
        "        # Update weights and biases\n",
        "        for l in range(1, self.num_hidden_layers + 1):\n",
        "          m_w[l] = (beta1 * m_w[l]) + (1-beta1) * del_w[l]\n",
        "          m_b[l] = (beta1 * m_b[l]) + (1-beta1) * del_b[l]\n",
        "\n",
        "          v_w[l] = beta2 * v_w[l] + (1 - beta2) * (del_w[l] ** 2)\n",
        "          v_b[l] = beta2 * v_b[l] + (1 - beta2) * (del_b[l] ** 2)\n",
        "\n",
        "          m_w_hat[l] = m_w[l]/(1-np.power(beta1, l+1))\n",
        "          m_b_hat[l] = m_b[l]/(1-np.power(beta1, l+1))\n",
        "          v_w_hat[l] = v_w[l]/(1-np.power(beta2, l+1))\n",
        "          v_b_hat[l] = v_b[l]/(1-np.power(beta2, l+1))\n",
        "\n",
        "          #update parameters\n",
        "          weights[l] -= eta*m_w_hat[l]/(np.sqrt(v_w_hat[l])+eps)\n",
        "          biases[l] -= eta*m_b_hat[l]/(np.sqrt(v_b_hat[l])+eps)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          accuracy = self.get_accuracy(self.get_predictions(pred_output), self.y_input)\n",
        "          print(f\"Iteration: {epoch}, Accuracy: {accuracy}\")\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  fashion_mnist = keras.datasets.fashion_mnist\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  x_train_norm = x_train / 255\n",
        "  x_test_norm = x_test / 255\n",
        "\n",
        "  # Define hyperparameters\n",
        "  num_of_pixels = 28 * 28                                                         #28 * 28 = 784 pixels\n",
        "  hidden_neurons_list = [128, 128]\n",
        "  num_hidden_layers = len(hidden_neurons_list)\n",
        "  output_neurons = 10\n",
        "  eta = 1e-2\n",
        "  epochs = 51\n",
        "  activation_function = \"sigmoid\"\n",
        "  initialization = \"normal\"\n",
        "  batch_size = 1\n",
        "  beta1 = 0.999\n",
        "  beta2 = 0.9\n",
        "  eps = 1e-10\n",
        "\n",
        "  #Taking pixels as inputs\n",
        "  x_train_input = x_train_norm.reshape(len(x_train_norm), num_of_pixels)                      #flattening the image into 1d array\n",
        "  x_test_input = x_test_norm.reshape(len(x_test_norm), num_of_pixels)                         #same thing\n",
        "  x_train_input = x_train_input.T\n",
        "  x_test_input = x_test_input.T\n",
        "\n",
        "  # Neural network class -> nn object\n",
        "  nn = NeuralNetwork(x_test_input, y_test, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons)\n",
        "\n",
        "  # Call the gradient_descent method\n",
        "  #weights, biases = nn.gradient_descent(epochs, eta, activation_function, initialization)\n",
        "  #weights, biases = nn.stochastic_gradient_descent(epochs, eta, activation_function, initialization, batch_size)\n",
        "  #weights, biases = nn.momentum_based_gradient_descent(epochs, eta, beta, activation_function, initialization)\n",
        "  #weights, biases = nn.nesterov_accelerated_gradient_descent(epochs, eta, beta, activation_function, initialization)\n",
        "  #weights, biases = nn.adagrad_gradient_descent(epochs, eta, eps, activation_function, initialization)\n",
        "  #weights, biases = nn.rmsProp_gradient_descent(epochs, eta, eps, beta, activation_function, initialization)\n",
        "  weights, biases = nn.adam_gradient_descent(epochs, eta, eps, beta1, beta2, activation_function, initialization)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}