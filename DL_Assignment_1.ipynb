{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcFOc4/7TbxaOFA3eRvGdI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravKanwat/DL-CS6910/blob/main/DL_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "LlvKesVaMN4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "\n",
        "import sys\n",
        "sys.path.append('Assignment_1')\n",
        "import hyperparameter_config\n",
        "wandb.login(key=\"Your-API-Key\")"
      ],
      "metadata": {
        "id": "HD7cQlG1MEMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def printImages(x_train, y_train):\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  index = [list(y_train).index(i) for i in range(len(classes))]\n",
        "\n",
        "  # image --> An image in a class; labels --> label\n",
        "  images = []\n",
        "  labels = []\n",
        "  for i in index:\n",
        "    images.append(x_train[i])\n",
        "    labels.append(classes[y_train[i]])\n",
        "  wandb.log({\"Images\": [wandb.Image(image, caption=caption) for image, caption in zip(images, labels)]}, step=i)"
      ],
      "metadata": {
        "id": "G8nn2JMq-Dmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lx4k0rtLztlX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons):\n",
        "      self.num_of_pixels = num_of_pixels\n",
        "      self.hidden_neurons_list = hidden_neurons_list\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.output_neurons = output_neurons\n",
        "\n",
        "\n",
        "    def initialize_parameters(self, num_of_pixels, hidden_neurons_list, num_hidden_layers, output_neurons, initialization):\n",
        "\n",
        "      weights = {}\n",
        "      biases = {}\n",
        "      prev_weights = {}\n",
        "      prev_biases = {}\n",
        "\n",
        "      # Xavier initialization\n",
        "      if initialization == \"Xavier\":\n",
        "        weights[0] = np.random.randn(hidden_neurons_list[0], num_of_pixels) * np.sqrt(1 / num_of_pixels)\n",
        "        biases[0] = np.zeros((hidden_neurons_list[0], 1))\n",
        "\n",
        "        # Initialize weights and biases for hidden layers\n",
        "        for l in range(1, len(hidden_neurons_list)):\n",
        "          weights[l] = np.random.randn(hidden_neurons_list[l], hidden_neurons_list[l-1]) * np.sqrt(1 / hidden_neurons_list[l-1])\n",
        "          biases[l] = np.zeros((hidden_neurons_list[l], 1))\n",
        "\n",
        "        # Initialize weights for last hidden layer to output layer\n",
        "        weights[len(hidden_neurons_list)] = np.random.randn(output_neurons, hidden_neurons_list[-1]) * np.sqrt(1 / hidden_neurons_list[-1])\n",
        "        biases[len(hidden_neurons_list)] = np.zeros((output_neurons, 1))\n",
        "\n",
        "        # Initialize previous weights and biases\n",
        "        for l in range(num_hidden_layers + 1):\n",
        "          prev_weights[l] = np.zeros_like(weights[l])\n",
        "          prev_biases[l] = np.zeros_like(biases[l])\n",
        "\n",
        "        return weights, biases, prev_weights, prev_biases\n",
        "\n",
        "      #random initialization\n",
        "      else:\n",
        "        weights[0] = np.random.rand(hidden_neurons_list[0], num_of_pixels) - 0.5\n",
        "        biases[0] = np.random.rand(hidden_neurons_list[0], 1) - 0.5\n",
        "        for l in range(num_hidden_layers):\n",
        "          weights[l] = np.random.rand(hidden_neurons_list[l], num_of_pixels if l == 0 else hidden_neurons_list[l-1]) - 0.5\n",
        "          biases[l] = np.random.rand(hidden_neurons_list[l], 1) - 0.5\n",
        "        weights[num_hidden_layers] = np.random.rand(output_neurons, hidden_neurons_list[-1]) - 0.5\n",
        "        biases[num_hidden_layers] = np.random.rand(output_neurons, 1) - 0.5\n",
        "\n",
        "        for l in range(num_hidden_layers + 1):\n",
        "          prev_weights[l] = np.zeros_like(weights[l])\n",
        "          prev_biases[l] = np.zeros_like(biases[l])\n",
        "      return weights, biases, prev_weights, prev_biases\n",
        "      '''\n",
        "          Initializing the weights and biases, both are dictionary which are storing random values generated by rand between (0 to 1) and subtracting 0.5 from it makes it between\n",
        "          -0.5 to 0.5\n",
        "      '''\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "      sigmoid_x = np.where(x < -30, 1, 1 / (1 + np.exp(-x)))\n",
        "      return sigmoid_x\n",
        "\n",
        "    def reLU(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def tanh(self, x):\n",
        "      return np.tanh(x)\n",
        "\n",
        "    def identity(self, x):\n",
        "      return x\n",
        "\n",
        "    def softmax(self, x):\n",
        "\n",
        "        max_x = np.max(x, axis=0)\n",
        "\n",
        "        # avoiding overflow\n",
        "        exp_x = np.exp(x - max_x)\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "\n",
        "    def feedforward_propagation(self, X, weights, biases, num_hidden_layers, activation_function):\n",
        "      a = []\n",
        "      h = []\n",
        "\n",
        "      for k in range(num_hidden_layers):\n",
        "\n",
        "        if k == 0:\n",
        "\n",
        "          a.append(np.dot(weights[k], X) + biases[k])\n",
        "          if(activation_function == \"reLU\"):\n",
        "            h.append(self.reLU(a[k]))\n",
        "          elif(activation_function == \"sigmoid\"):\n",
        "            h.append(self.sigmoid(a[k]))\n",
        "          elif(activation_function == \"tanh\"):\n",
        "            h.append(self.tanh(a[k]))\n",
        "          elif(activation_function == \"identity\"):\n",
        "            h.append(self.identity(a[k]))\n",
        "\n",
        "        else:\n",
        "\n",
        "          a.append(np.dot(weights[k], h[k-1]) + biases[k])\n",
        "          if(activation_function == \"reLU\"):\n",
        "            h.append(self.reLU(a[k]))\n",
        "          elif(activation_function == \"sigmoid\"):\n",
        "            h.append(self.sigmoid(a[k]))\n",
        "          elif(activation_function == \"tanh\"):\n",
        "            h.append(self.tanh(a[k]))\n",
        "          elif(activation_function == \"identity\"):\n",
        "            h.append(self.identity(a[k]))\n",
        "\n",
        "\n",
        "      a.append(np.dot(weights[num_hidden_layers], h[num_hidden_layers - 1]) + biases[num_hidden_layers])\n",
        "      y_hat = self.softmax(a[-1])\n",
        "      return a, h, y_hat\n",
        "\n",
        "    def one_hot(self, Y):\n",
        "      if Y.max() != 9:\n",
        "        one_hot_Y = np.zeros((Y.size, 10))\n",
        "      else:\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "      one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "      one_hot_Y = one_hot_Y.T\n",
        "      return one_hot_Y\n",
        "\n",
        "    def deriv_sigmoid(self, Z):\n",
        "      func = self.sigmoid(Z)\n",
        "      return func * (1 - func)\n",
        "\n",
        "    def deriv_reLU(self, Z):\n",
        "      return Z > 0\n",
        "\n",
        "    def deriv_tanh(self, x):\n",
        "      return 1 - np.tanh(x)**2\n",
        "\n",
        "    def deriv_identity(self, x):\n",
        "      return 1\n",
        "\n",
        "    def back_propagation(self, Y, fwd_A, fwd_H, weights, biases, pred_output, num_hidden_layers, activation_function):\n",
        "      one_hot_Y = self.one_hot(Y)\n",
        "      dA = {}\n",
        "      dH = {}\n",
        "      dW = {}\n",
        "      dB = {}\n",
        "\n",
        "      dA[num_hidden_layers] = pred_output - one_hot_Y\n",
        "\n",
        "      for k in range(num_hidden_layers, 0, -1):\n",
        "        dW[k] = np.dot(dA[k], fwd_H[k-1].T)\n",
        "        dB[k] = np.mean(dA[k], axis=1, keepdims=True)\n",
        "\n",
        "        dH[k-1] = np.dot(weights[k].T, dA[k])\n",
        "        if(activation_function == \"reLU\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_reLU(fwd_A[k-1]))\n",
        "        elif(activation_function == \"sigmoid\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_sigmoid(fwd_A[k-1]))\n",
        "        elif(activation_function == \"tanh\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_tanh(fwd_A[k-1]))\n",
        "        elif(activation_function == \"identity\"):\n",
        "          dA[k-1] = np.multiply(dH[k-1], self.deriv_identity(fwd_A[k-1]))\n",
        "      return dW, dB\n",
        "\n",
        "    def get_predictions(self, pred_output):\n",
        "      return np.argmax(pred_output, axis = 0)\n",
        "\n",
        "    def get_accuracy(self, y_pred, y_true):\n",
        "      return np.sum(y_pred == y_true) / y_true.size\n",
        "\n",
        "\n",
        "    def loss_function(self, y_pred, y_true, loss, weights, weight_decay):\n",
        "\n",
        "      #Cross Entropy\n",
        "      if(loss == 'cross_entropy'):\n",
        "        epsilon = 1e-30\n",
        "        cross_entropy_loss = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=0))\n",
        "\n",
        "        # L2 Regularisation\n",
        "        reg_loss = 0.5 * weight_decay * sum(np.sum(w ** 2) for w in weights.values())\n",
        "        total_loss = cross_entropy_loss + reg_loss\n",
        "\n",
        "      #Mean Squared Error\n",
        "      elif(loss == 'mse'):\n",
        "        mse_loss = np.mean(np.sum((y_pred - y_true) ** 2))\n",
        "\n",
        "        # L2 Regularisation\n",
        "        reg_loss = 0.5 * weight_decay * sum(np.sum(w ** 2) for w in weights.values())\n",
        "        total_loss = mse_loss + reg_loss\n",
        "      return total_loss\n",
        "\n",
        "\n",
        "    # Gradient descent and Optimizers\n",
        "\n",
        "    def gradient_descent(self, weights, biases, dW, dB, eta):\n",
        "\n",
        "      # Update weights and biases\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        weights[l] -= eta * dW[l]\n",
        "        biases[l] -= eta * dB[l]\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def momentum_based_gradient_descent(self, weights, biases, prev_weights, prev_biases, dW, dB, eta, momentum):\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        uw = momentum * prev_weights[l] + eta * dW[l]\n",
        "        ub = momentum * prev_biases[l] + eta * dB[l]\n",
        "\n",
        "        # Update current and prev weights and biases\n",
        "        weights[l] -= uw\n",
        "        biases[l] -= ub\n",
        "        prev_weights[l] = uw\n",
        "        prev_biases[l] = ub\n",
        "      return weights, biases, prev_weights, prev_biases\n",
        "\n",
        "\n",
        "    def adagrad_gradient_descent(self, weights, biases, dW, dB, eta, eps):\n",
        "\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = v_w[l] + dW[l]**2\n",
        "        v_b[l] = v_b[l] + dB[l]**2\n",
        "\n",
        "        # Update weights and biases\n",
        "        weights[l] -= eta * dW[l] / (np.sqrt(v_w[l]) + eps)\n",
        "        biases[l] -= eta * dB[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def rmsProp_gradient_descent(self, weights, biases, dW, dB, eta, eps, beta):\n",
        "\n",
        "      v_w = {}\n",
        "      v_b = {}\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = 0\n",
        "        v_b[l] = 0\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        v_w[l] = (beta * v_w[l]) + ((1-beta) * dW[l] ** 2)\n",
        "        v_b[l] = (beta * v_b[l]) + ((1-beta) * dB[l] ** 2)\n",
        "\n",
        "        # Update weights and biases\n",
        "        weights[l] -= eta * dW[l] / (np.sqrt(v_w[l]) + eps)\n",
        "        biases[l] -= eta * dB[l] / (np.sqrt(v_b[l]) + eps)\n",
        "\n",
        "      return weights, biases\n",
        "\n",
        "\n",
        "    def adam_gradient_descent(self, weights, biases, ts, v_w, v_b, m_w, m_b, dW, dB, eta, eps, beta1, beta2):\n",
        "\n",
        "      for l in range(1, self.num_hidden_layers + 1):\n",
        "        mdW = (beta1 * m_w[l]) + (1-beta1) * dW[l]\n",
        "        mdB = (beta1 * m_b[l]) + (1-beta1) * dB[l]\n",
        "\n",
        "        vdW = beta2 * v_w[l] + (1 - beta2) * (dW[l] ** 2)\n",
        "        vdB = beta2 * v_b[l] + (1 - beta2) * (dB[l] ** 2)\n",
        "\n",
        "        m_w_hat = mdW/(1-np.power(beta1, ts))\n",
        "        v_w_hat = vdW/(1-np.power(beta2, ts))\n",
        "        m_b_hat = mdB/(1-np.power(beta1, ts))\n",
        "        v_b_hat = vdB/(1-np.power(beta2, ts))\n",
        "\n",
        "        #update weights and biases\n",
        "        weights[l] -= eta*m_w_hat/(np.sqrt(v_w_hat+eps))\n",
        "        biases[l] -= eta*m_b_hat/(np.sqrt(v_b_hat+eps))\n",
        "\n",
        "        v_w[l] = vdW\n",
        "        v_b[l] = vdB\n",
        "        m_w[l] = mdW\n",
        "        m_b[l] = mdB\n",
        "\n",
        "      ts += 1\n",
        "\n",
        "      return weights, biases, v_w, v_b, m_w, m_b, ts\n",
        "\n",
        "    # <---------------------------------------------START--------------------------------------------------->\n",
        "\n",
        "\n",
        "    ''' Add new optimizers here '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <---------------------------------------------END----------------------------------------------------->\n",
        "\n",
        "\n",
        "\n",
        "    def compute_accuracy(self, X_test, y_test, weights, biases, num_hidden_layers, activation_function):\n",
        "\n",
        "      _, _, pred_output = self.feedforward_propagation(X_test, weights, biases, num_hidden_layers, activation_function)\n",
        "      pred_labels = np.argmax(pred_output, axis=0)\n",
        "      accuracy = np.mean(pred_labels == y_test)\n",
        "      return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_neural_network(nn, x_train_input, y_train, x_test_input, y_test, x_val, y_val, weights, biases, prev_weights, prev_biases, num_hidden_layers, activation_function, optimizer, epochs, batch_size, eta, momentum, beta, beta1, beta2, eps, weight_decay, loss):\n",
        "\n",
        "  data_size = len(x_train_input[0])\n",
        "\n",
        "  if optimizer == \"sgd\":\n",
        "    batch_size = 1\n",
        "\n",
        "  lookahead_w = weights\n",
        "  lookahead_b = biases\n",
        "  ts = 1\n",
        "  v_w = prev_weights.copy()\n",
        "  v_b = prev_biases.copy()\n",
        "  m_w = prev_weights.copy()\n",
        "  m_b = prev_biases.copy()\n",
        "\n",
        "  for iter in tqdm(range(epochs)):\n",
        "    total_train_loss = 0\n",
        "    for i in range(0, data_size, batch_size):\n",
        "      if i<= data_size - batch_size:\n",
        "        X_batch = x_train_input[:, i:i+batch_size]\n",
        "        Y_batch = y_train[i:i+batch_size]\n",
        "\n",
        "        if optimizer == \"sgd\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.gradient_descent(weights, biases, dW, dB, eta)\n",
        "\n",
        "        elif optimizer == \"momentum\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases, _, _ = nn.momentum_based_gradient_descent(weights, biases, prev_weights, prev_biases, dW, dB, eta, momentum)\n",
        "\n",
        "        elif optimizer == \"nesterov\":\n",
        "\n",
        "          # Partial updates\n",
        "          for l in range(1, num_hidden_layers+1):\n",
        "            lookahead_w[l] = weights[l] - beta * prev_weights[l]\n",
        "            lookahead_b[l] = biases[l] - beta * prev_biases[l]\n",
        "\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, lookahead_w, lookahead_b, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, lookahead_w, lookahead_b, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases, prev_weights, prev_biases = nn.momentum_based_gradient_descent(weights, biases, prev_weights, prev_biases, dW, dB, epochs, eta, beta)\n",
        "\n",
        "        elif optimizer == \"rmsProp\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases = nn.rmsProp_gradient_descent(weights, biases, dW, dB, eta, eps, beta)\n",
        "\n",
        "        elif optimizer == \"adam\":\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, weights, biases, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases, v_w, v_b, m_w, m_b, ts = nn.adam_gradient_descent(weights, biases, ts, v_w, v_b, m_w, m_b, dW, dB, eta, eps, beta1, beta2)\n",
        "\n",
        "        elif optimizer == \"nadam\":\n",
        "\n",
        "          # Partial updates\n",
        "          for l in range(1, num_hidden_layers+1):\n",
        "            lookahead_w[l] = weights[l] - beta * prev_weights[l]\n",
        "            lookahead_b[l] = biases[l] - beta * prev_biases[l]\n",
        "\n",
        "          fwd_a, fwd_h, pred_output = nn.feedforward_propagation(X_batch, lookahead_w, lookahead_b, num_hidden_layers, activation_function)\n",
        "\n",
        "          one_hot_Y = nn.one_hot(Y_batch)\n",
        "          train_loss = nn.loss_function(pred_output, one_hot_Y, loss, weights, weight_decay)\n",
        "          total_train_loss += train_loss\n",
        "\n",
        "          dW, dB = nn.back_propagation(Y_batch, fwd_a, fwd_h, lookahead_w, lookahead_b, pred_output, num_hidden_layers, activation_function)\n",
        "          weights, biases, v_w, v_b, m_w, m_b, ts = nn.adam_gradient_descent(weights, biases, ts, v_w, v_b, m_w, m_b, dW, dB, eta, eps, beta1, beta2)\n",
        "\n",
        "    # <---------------------------------------START---------------------------------------------->\n",
        "\n",
        "\n",
        "    ''' Call new optimizer here '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # <---------------------------------------END------------------------------------------------>\n",
        "\n",
        "    avg_train_loss = total_train_loss / (data_size / batch_size)\n",
        "\n",
        "    _, _, val_pred = nn.feedforward_propagation(x_val, weights, biases, num_hidden_layers, activation_function)\n",
        "    val_one_hot = nn.one_hot(y_val)\n",
        "    val_loss = nn.loss_function(val_pred, val_one_hot, loss, weights, weight_decay)\n",
        "    if(loss == 'mse'):\n",
        "      val_loss = val_loss / (data_size / batch_size)\n",
        "\n",
        "    _, _, test_pred = nn.feedforward_propagation(x_test_input, weights, biases, num_hidden_layers, activation_function)\n",
        "    test_one_hot = nn.one_hot(y_test)\n",
        "    test_loss = nn.loss_function(test_pred, test_one_hot, loss, weights, weight_decay)\n",
        "    if(loss == 'mse'):\n",
        "      test_loss = test_loss / (data_size / batch_size)\n",
        "\n",
        "    val_accuracy = nn.compute_accuracy(x_val, y_val, weights, biases, num_hidden_layers, activation_function)\n",
        "    train_accuracy = nn.compute_accuracy(x_train_input, y_train, weights, biases, num_hidden_layers, activation_function)\n",
        "    test_accuracy = nn.compute_accuracy(x_test_input, y_test, weights, biases, num_hidden_layers, activation_function)\n",
        "\n",
        "    print(f\"val accuracy: {val_accuracy * 100:.2f}%, Test accuracy : {test_accuracy * 100:.2f}, Val loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "    wandb.log({'val_accuracy' : val_accuracy * 100, 'accuracy' : train_accuracy * 100, 'test_accuracy' : test_accuracy * 100,\n",
        "               'loss' : avg_train_loss, 'val loss' : val_loss, 'test_loss' : test_loss, 'epoch' : iter}, step=iter)\n",
        "\n",
        "  return weights, biases"
      ],
      "metadata": {
        "id": "C_lwR40u95oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def configParse():\n",
        "    parser = argparse.ArgumentParser(description='Train neural network with specified parameters.')\n",
        "    parser.add_argument('--wandb_project', type = str, default = 'Testing', help = 'project name')\n",
        "    parser.add_argument('--wandb_entity', type = str, default='Test Accuracy', help = 'wandb entity')\n",
        "    parser.add_argument('--dataset', type = str, default = 'fashion_mnist', help = 'dataset')\n",
        "    parser.add_argument('--epochs', type = int, default = 10, help='epochs')\n",
        "    parser.add_argument('--batch_size', type = int, default = 64, help='batch size')\n",
        "    parser.add_argument('--loss', type=str, default = 'cross_entropy', help='loss function')\n",
        "    parser.add_argument('--optimizer', type=str, default = 'nadam', help='optimizer')\n",
        "    parser.add_argument('--learning_rate', type=float, default = 1e-3, help='learning rate')\n",
        "    parser.add_argument('--momentum', type=float, default = 0.9, help='Momentum')\n",
        "    parser.add_argument('--beta', type=float, default = 0.9, help='beta')\n",
        "    parser.add_argument('--beta1', type=float, default = 0.9, help='beta1')\n",
        "    parser.add_argument('--beta2', type=float, default = 0.999, help='beta2')\n",
        "    parser.add_argument('--epsilon', type=float, default = 1e-8, help='epsilon')\n",
        "    parser.add_argument('--weight_decay', type=float, default = 0.0, help='weight decay')\n",
        "    parser.add_argument('--weight_init', type=str, default = \"Xavier\", help='weight initialization')\n",
        "    parser.add_argument('--num_layers', type=int, default = 3, help='number of hidden layers')\n",
        "    parser.add_argument('--hidden_size', type=int, default = 128, help='size of a hidden layer')\n",
        "    parser.add_argument('--activation', type=str, default = \"tanh\", help='activation function')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args"
      ],
      "metadata": {
        "id": "kwOWrUNg-Shz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neural_network import NeuralNetwork, train_neural_network\n",
        "\n",
        "def printImages(x_train, y_train):\n",
        "  classes = {0:\"T-shirt/top\", 1:\"Trouser\", 2:\"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle Boot\"}\n",
        "  index = [list(y_train).index(i) for i in range(len(classes))]\n",
        "\n",
        "  # image --> An image in a class; labels --> label\n",
        "  images = []\n",
        "  labels = []\n",
        "  for i in index:\n",
        "    images.append(x_train[i])\n",
        "    labels.append(classes[y_train[i]])\n",
        "  wandb.log({\"Images\": [wandb.Image(image, caption=caption) for image, caption in zip(images, labels)]}, step=i)\n",
        "\n",
        "def main(args):\n",
        "\n",
        "  # Taking dataset according to parameters passed by user\n",
        "  if(args.dataset == \"fashion_mnist\"):\n",
        "    fashion_mnist = keras.datasets.fashion_mnist\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  elif(args.dataset == \"mnist\"):\n",
        "    mnist = keras.datasets.mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "  # Train test split using sklearn library\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=10000, random_state=42)\n",
        "\n",
        "  #Labels of the dataset\n",
        "  class_names = []\n",
        "  if(args.dataset == \"fashion_mnist\"):\n",
        "    class_names=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "  elif(args.dataset == \"mnist\"):\n",
        "    class_names = [\"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", \"Nine\"]\n",
        "\n",
        "  # Normalizing the pixels to avoid overflow\n",
        "  x_train_norm = x_train / 255\n",
        "  x_test_norm = x_test / 255\n",
        "  x_val_norm = x_val / 255\n",
        "\n",
        "  # 28 * 28 = 784 pixels\n",
        "  num_of_pixels = 28 * 28\n",
        "\n",
        "  output_neurons = 10\n",
        "\n",
        "  #Taking pixels as inputs\n",
        "  x_train_input = x_train_norm.reshape(len(x_train_norm), num_of_pixels)                      #flattening the image into 1d array\n",
        "  x_test_input = x_test_norm.reshape(len(x_test_norm), num_of_pixels)                         #same thing\n",
        "  x_val_reshape = x_val_norm.reshape(len(x_val_norm), num_of_pixels)\n",
        "\n",
        "  # Taking transpose of the dataset, so it becomes 784 x 50000 meaning each column represents an image\n",
        "  x_train_input = x_train_input.T\n",
        "  x_test_input = x_test_input.T\n",
        "  x_val = x_val_reshape.T\n",
        "\n",
        "\n",
        "  # Define hyperparameters\n",
        "  sweep_config = {\n",
        "     'method' : 'random',\n",
        "     'project' : args.wandb_project,\n",
        "     'name' : 'Test Accuracy and Confusion Matrix',\n",
        "     'entity' : args.wandb_entity,\n",
        "     'metric' : {\n",
        "        'name' : 'val_accuracy',\n",
        "        'goal' : 'maximize',\n",
        "     },\n",
        "     'parameters' : {\n",
        "        'eta' : {\n",
        "           'values' : [args.learning_rate]\n",
        "        },\n",
        "        'epochs' : {\n",
        "           'values' : [args.epochs]\n",
        "        },\n",
        "        'num_hidden_layers' : {\n",
        "           'values' : [args.num_layers]\n",
        "        },\n",
        "        'num_hidden_neurons' : {\n",
        "           'values' : [args.hidden_size]\n",
        "        },\n",
        "        'activation_function' : {\n",
        "           'values' : [args.activation]\n",
        "        },\n",
        "        'initialization' : {\n",
        "           'values' : [args.weight_init]\n",
        "        },\n",
        "        'optimizer' : {\n",
        "           'values' : [args.optimizer]\n",
        "        },\n",
        "        'batch_size' : {\n",
        "           'values' : [args.batch_size]\n",
        "        },\n",
        "        'momentum' : {\n",
        "           'values' : [args.momentum]\n",
        "        },\n",
        "        'beta' : {\n",
        "           'values' : [args.beta]\n",
        "        },\n",
        "        'beta1' : {\n",
        "           'values' : [args.beta1]\n",
        "        },\n",
        "        'beta2' : {\n",
        "           'values' : [args.beta2]\n",
        "        },\n",
        "        'eps' : {\n",
        "           'values' : [args.epsilon]\n",
        "        },\n",
        "        'weight_decay' : {\n",
        "           'values' : [args.weight_decay]\n",
        "        },\n",
        "        'loss' : {\n",
        "           'values' : [args.loss]\n",
        "        }\n",
        "     }\n",
        "  }\n",
        "\n",
        "\n",
        "  run_name = \"\"\n",
        "\n",
        "  def train():\n",
        "    with wandb.init(project = args.wandb_project, entity = args.wandb_entity) as run:\n",
        "\n",
        "      # Creates names of runs based on parameters. Example => hl_4_bs_64_ac_reLU\n",
        "      config = wandb.config\n",
        "      run_name = \"hl_\" + str(config.num_hidden_layers) + \"_bs_\" + str(config.batch_size) + \"_ac_\" + config.activation_function\n",
        "      wandb.run.name = run_name\n",
        "\n",
        "    #   printImages(x_train, y_train)           ---> run when want to print the images\n",
        "\n",
        "\n",
        "      # creating the list of hidden_neurons\n",
        "      hidden_neurons_list = []\n",
        "      for i in range(config.num_hidden_layers):\n",
        "        hidden_neurons_list.append(config.num_hidden_neurons)\n",
        "\n",
        "      # Creates an object of class NeuralNetwork and Initializes the parameters\n",
        "      nn = NeuralNetwork(num_of_pixels, hidden_neurons_list, config.num_hidden_layers, output_neurons)\n",
        "      weights, biases, prev_weights, prev_biases = nn.initialize_parameters(num_of_pixels, hidden_neurons_list, config.num_hidden_layers, output_neurons, config.initialization)\n",
        "\n",
        "      # Train the network\n",
        "      weights, biases = train_neural_network(nn, x_train_input, y_train, x_test_input, y_test, x_val, y_val, weights, biases, prev_weights, prev_biases, config.num_hidden_layers, config.activation_function,\n",
        "                                             config.optimizer, config.epochs, config.batch_size, config.eta, config.momentum, config.beta, config.beta1, config.beta2, config.eps, config.weight_decay, config.loss)\n",
        "\n",
        "\n",
        "      # Print the confusion matrix\n",
        "      _, _, y_test_pred = nn.feedforward_propagation(x_test_input, weights, biases, config.num_hidden_layers, config.activation_function)\n",
        "      y_test_pred = np.argmax(y_test_pred, axis=0)\n",
        "      conf_matrix = wandb.plot.confusion_matrix(y_true = y_test, preds = y_test_pred, class_names = class_names)\n",
        "      wandb.sklearn.plot_confusion_matrix(y_test, y_test_pred, class_names)\n",
        "\n",
        "\n",
        "  sweep_id = wandb.sweep(sweep=sweep_config)\n",
        "  wandb.agent(sweep_id, function=train,count=1)\n",
        "  wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = hyperparameter_config.configParse()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "id": "S6PJn-7E_gdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}